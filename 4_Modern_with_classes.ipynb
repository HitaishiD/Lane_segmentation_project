{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import segmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Color Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping RGB colors to class labels\n",
    "# The key is the (R, G, B) tuple, and the value is the corresponding class label\n",
    "color_map = {\n",
    "    (128, 64, 128): 1,    # Road\n",
    "    (244,35,232): 2,      # Sidewalk\n",
    "    (70, 70, 70): 3,      # Building\n",
    "    (102,102,156): 4,     # Wall\n",
    "    (190, 153, 153): 5,   # Fence\n",
    "    (107, 142, 35): 6,    # Vegetation\n",
    "    (152,251,125): 7,     # Terrain\n",
    "    (70, 130, 180): 8,    # Sky\n",
    "    (220, 20, 60): 9,     # Person\n",
    "    (0, 0, 142): 10,      # Car\n",
    "    (119, 11, 32): 11,    # Bicycle\n",
    "    (0, 0, 230): 12,      # Motorcycle\n",
    "    (0,0,0): 0,           # Background (can be 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def convert_rgb_to_mask(self, mask_folder, output_mask_folder, color_map):\n",
    "        ''' convert a folder of rgb masks to single channel mask with class indices'''\n",
    "        os.makedirs(output_mask_folder, exist_ok=True)\n",
    "\n",
    "        lookup_table = np.zeros((256, 256, 256), dtype=np.uint8)\n",
    "        for (r, g, b), label in color_map.items():\n",
    "            lookup_table[r, g, b] = label\n",
    "\n",
    "        mask_files = [f for f in os.listdir(mask_folder) if f.endswith(\"png\")]\n",
    "\n",
    "        for mask_filename in tqdm(mask_files, desc=\"Processing Masks\", unit=\"img\"):\n",
    "\n",
    "            mask_path = os.path.join(mask_folder, mask_filename)\n",
    "            mask_image = cv2.imread(mask_path)\n",
    "            mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            integer_mask = lookup_table[mask_image[...,0], mask_image[...,1], mask_image[...,2]]\n",
    "    \n",
    "            output_mask_path = os.path.join(output_mask_folder, mask_filename)\n",
    "            cv2.imwrite(output_mask_path, integer_mask)     \n",
    "\n",
    "\n",
    "    def convert_mask_to_rgb(mask, color_map):\n",
    "        ''' convert an 2D mask with class indices to an rgb mask '''\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.cpu().numpy()  \n",
    "\n",
    "        height, width = mask.shape\n",
    "        rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "        for class_rgb, class_idx in color_map.items():\n",
    "            rgb_mask[mask == class_idx] = class_rgb  \n",
    "\n",
    "        return rgb_mask           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Masks: 100%|██████████| 200/200 [00:01<00:00, 105.16img/s]\n"
     ]
    }
   ],
   "source": [
    "# Test Preprocessor class\n",
    "\n",
    "mask_folder = '/home/ubuntu/computer-vision/computer-vision/training/semantic_rgb'\n",
    "output_mask_folder = '/home/ubuntu/computer-vision/computer-vision/testing_preprocessed_masks'\n",
    "\n",
    "preprocessor = Processor()\n",
    "preprocessor.convert_rgb_to_mask(mask_folder, output_mask_folder, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensorWithoutNormalization:\n",
    "    def __call__(self, sample):\n",
    "        mask = np.array(sample)\n",
    "        mask_tensor = torch.tensor(mask, dtype=torch.long)\n",
    "        return mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIdataset(Dataset):\n",
    "    def __init__(self,image_dir, mask_dir, transform=None, mask_transform=None, image_size=(256, 256)):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.image_names = sorted([f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.mask_names = sorted([f for f in os.listdir(mask_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "        assert len(self.image_names) == len(self.mask_names), \"Mismatch between image and mask count!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_names[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_names[idx])\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.image_size:\n",
    "            image = image.resize(self.image_size)\n",
    "            mask = mask.resize(self.image_size)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset class and Transforms class\n",
    "\n",
    "image_dir = '/home/ubuntu/computer-vision/computer-vision/training/image_2'\n",
    "mask_dir = '/home/ubuntu/computer-vision/computer-vision/testing_preprocessed_masks'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    ToTensorWithoutNormalization()\n",
    "])\n",
    "\n",
    "dataset = KITTIdataset(image_dir=image_dir, mask_dir=mask_dir,\n",
    "                       transform=transform, mask_transform=mask_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.model = segmentation.deeplabv3_resnet101(weights='DEFAULT')\n",
    "        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Test Model class\n",
    "NUM_CLASSES = 13\n",
    "\n",
    "model = DeepLabV3Plus(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Not sure what to do with this cell\n",
    "num_img = dataset.__len__()\n",
    "BATCH_SIZE = 2\n",
    "CHECKPOINT_DIR = \"experiments\"\n",
    "EXPERIMENT_NAME = '5epochs_local_training'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_size = int(0.6*num_img)\n",
    "val_size = int(0.2*num_img)\n",
    "test_size = num_img - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class (To do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert into class\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    logging.info(\"Starting training...\")\n",
    "\n",
    "    epoch_progress = tqdm(range(epochs), desc=\"Training Progress\")  \n",
    "\n",
    "    for epoch in epoch_progress:\n",
    "        print(f\"\\n Epoch {epoch + 1}/{epochs}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Train Phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device),  masks.squeeze(1).long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        logging.info(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device),  masks.squeeze(1).long().to(device)\n",
    "\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, masks)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "        logging.info(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        epoch_progress.set_postfix({\"Train Loss\": f\"{avg_train_loss:.4f}\", \"Val Loss\": f\"{avg_val_loss:.4f}\"})\n",
    "\n",
    "        # Update plot\n",
    "        ax.clear()\n",
    "        ax.plot(train_losses, label=\"Train Loss\")\n",
    "        ax.plot(val_losses, label=\"Val Loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend()\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save model at the end of training\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, EXPERIMENT_NAME )\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Final model checkpoint saved!\")\n",
    "    logging.info(f\"Final model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # Save the losses to a CSV file\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Epoch': range(1, epochs+1),\n",
    "        'Training Loss': train_losses,\n",
    "        'Validation Loss': val_losses\n",
    "    })\n",
    "\n",
    "    loss_df.to_csv(EXPERIMENT_NAME, index = 'False')\n",
    "\n",
    "    print(\"Training Completed!\")\n",
    "    logging.info(\"Training Completed!\")\n",
    "    plt.ioff()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def compute_iou(self, pred_mask_array, true_mask_array, class_id, color_map):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes the Intersection over Union (IoU) for a specific class between \n",
    "        the predicted and true segmentation masks.\n",
    "\n",
    "        Parameters:\n",
    "            pred_mask_array (numpy.ndarray): The predicted mask 3D array (H, W, 3) \n",
    "            true_mask_array (numpy.ndarray): The ground truth mask image 3D array (H, W, 3) \n",
    "            class_id (int): The class ID for which to calculate the IoU.\n",
    "            color_map (dict): The dictionary mapping RGB colors to Class ID.                        \n",
    "\n",
    "        Returns:\n",
    "            float: The Intersection over Union (IoU) score for the specified class.\n",
    "                If there is no overlap (union is zero), returns 0.\"\"\"\n",
    "        \n",
    "        assert pred_mask_array.shape == true_mask_array.shape, \"Mask shapes should be the same\"\n",
    "        assert class_id in color_map.values(), \"class_ID should be defined in color to class mapping\"\n",
    "\n",
    "        class_rgb = np.array(list(key for key, value in color_map.items() if value == class_id)[0])\n",
    "\n",
    "        # Reshape the arrays into 2D arrays with 3 columns (RGB)\n",
    "        reshaped_pred_mask_array = pred_mask_array.reshape(-1, 3)\n",
    "        reshaped_true_mask_array = true_mask_array.reshape(-1, 3)\n",
    "\n",
    "        # Create binary masks (1 for the target class, 0 for others)\n",
    "        pred_lane = np.array([1 if np.all(pix == class_rgb) else 0 for pix in reshaped_pred_mask_array])\n",
    "        true_lane = np.array([1 if np.all(pix == class_rgb) else 0 for pix in reshaped_true_mask_array])\n",
    "\n",
    "        # Compute intersection and union\n",
    "        intersection = np.logical_and(pred_lane, true_lane).sum()\n",
    "        union = np.logical_or(pred_lane, true_lane).sum()\n",
    "\n",
    "        # Compute IoU \n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        return iou\n",
    "    \n",
    "    def compute_mean_iou(self,model, test_loader, class_id, color_map, device):\n",
    "        \"\"\"\n",
    "        Computes the mean IoU across all images in the test dataset for one class.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Trained segmentation model.\n",
    "            test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
    "            class_id (int): Class index for which the IoU needs to be computed\n",
    "            color_map (dict): Mapping from class indices to RGB values.\n",
    "            device (torch.device): Device (CPU/GPU).\n",
    "\n",
    "        Returns:\n",
    "            float: mean iou score.\"\"\"\n",
    "        \n",
    "        model.eval()  \n",
    "        all_iou = [] \n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for images, true_masks in test_loader:\n",
    "                images = images.to(device)  \n",
    "                true_masks = true_masks.to(device)  \n",
    "\n",
    "                # Get predictions from model\n",
    "                outputs = model(images)  # (B, num_classes, H, W)\n",
    "                predicted_masks = torch.argmax(outputs, dim=1)  #  (B, H, W)\n",
    "\n",
    "                for i in range(images.shape[0]):  \n",
    "                    # Convert predicted & true masks to RGB\n",
    "                    true_mask_rgb = Processor.convert_mask_to_rgb(true_masks[i].squeeze(0), color_map)\n",
    "                    pred_mask_rgb = Processor.convert_mask_to_rgb(predicted_masks[i], color_map)\n",
    "        \n",
    "                    iou = self.compute_iou(pred_mask_rgb, true_mask_rgb, class_id, color_map)\n",
    "                    all_iou.append(iou)\n",
    "\n",
    "        # Compute final mean IoU per class\n",
    "        mean_iou_score = sum(all_iou)/len(all_iou)\n",
    "\n",
    "        return mean_iou_score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\dhoow\\\\Desktop\\\\Computer Vision\\\\lane_segmentation_project\\\\experiments\\\\5epochs_local_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test Evaluator class\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# MODEL_PATH = os.path.join(CHECKPOINT_DIR, EXPERIMENT_NAME)\u001b[39;00m\n\u001b[32m      3\u001b[39m MODEL_PATH = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdhoow\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mComputer Vision\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlane_segmentation_project\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mexperiments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m5epochs_local_training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m evaluator = Evaluator()\n\u001b[32m      7\u001b[39m mean_iou = evaluator.compute_mean_iou(model, test_loader,\u001b[32m1\u001b[39m,color_map, DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\dhoow\\\\Desktop\\\\Computer Vision\\\\lane_segmentation_project\\\\experiments\\\\5epochs_local_training'"
     ]
    }
   ],
   "source": [
    "# Test Evaluator class\n",
    "# MODEL_PATH = os.path.join(CHECKPOINT_DIR, EXPERIMENT_NAME)\n",
    "#MODEL_PATH = r\"C:\\Users\\dhoow\\Desktop\\Computer Vision\\lane_segmentation_project\\experiments\\5epochs_local_training\"\n",
    "#model.load_state_dict(torch.load(MODEL_PATH, map_location = DEVICE))\n",
    "\n",
    "evaluator = Evaluator()\n",
    "mean_iou = evaluator.compute_mean_iou(model, test_loader,1,color_map, DEVICE)\n",
    "print(mean_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, model, color_map, device):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.color_map = color_map\n",
    "        \n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predict_from_image_path(self, image_path):\n",
    "        ''' Make prediction from image file'''\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.image_transform(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            predicted_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "        predicted_rgb_mask = Processor.convert_mask_to_rgb(predicted_mask, self.color_map)\n",
    "\n",
    "        return predicted_rgb_mask\n",
    "    \n",
    "    def predict_from_test_loader(self, test_loader):\n",
    "        ''' Make prediction for an image from test_loader'''\n",
    "\n",
    "        data_iter = iter(test_loader)\n",
    "        image, _ = next(data_iter)\n",
    "        image = image[0].unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image)\n",
    "            predicted_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "        predicted_rgb_mask = Processor.convert_mask_to_rgb(predicted_mask, self.color_map)\n",
    "\n",
    "        return predicted_rgb_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference class\n",
    "\n",
    "inference = Inference(model, color_map, DEVICE)\n",
    "image_path = r\"C:\\Users\\dhoow\\Desktop\\Computer Vision\\lane_segmentation_project\\dummy_kitti_dataset\\train_images\\000004_10.png\"\n",
    "inference.predict_from_image_path(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.predict_from_test_loader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotter class (To continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_image_from_path(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# plot image from image path\n",
    "\n",
    "# plot mask\n",
    "\n",
    "# plot image from data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plotter class\n",
    "\n",
    "image_path = r\"C:\\Users\\dhoow\\Desktop\\Computer Vision\\lane_segmentation_project\\dummy_kitti_dataset\\train_images\\000004_10.png\"\n",
    "plotter = Plotter()\n",
    "plotter.plot_image_from_path(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
