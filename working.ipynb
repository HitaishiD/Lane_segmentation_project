{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs8IYTGoZBzo"
      },
      "source": [
        "###### DeepLabV3+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5m7E92c5HZA"
      },
      "source": [
        "training images are in\n",
        "training/image_2 and labels are in training/semantic_rgb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gG50bnh4tjB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM3_nrGfVbhs"
      },
      "source": [
        "Change output classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMI5qK36x1Ah"
      },
      "source": [
        "###### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h8Is63ex2Ke",
        "outputId": "6e7cf4fa-a769-42a9-c107-4935b3d55144"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.segmentation import deeplabv3_resnet101\n",
        "\n",
        "# Load DeepLabV3+ with a ResNet-101 backbone\n",
        "model = deeplabv3_resnet101(pretrained=True)\n",
        "\n",
        "# Modify the last layer for 2 classes: {background, lane}\n",
        "model.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=1)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db1fZxo1x9qQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define a custom dataset class\n",
        "class KITTILaneDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
        "\n",
        "        image = cv2.imread(img_path)  # Load image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask (grayscale)\n",
        "\n",
        "        # Resize both the image and mask to a fixed size (e.g., 520x520)\n",
        "        image = cv2.resize(image, (520, 520))  # Resize image to 520x520\n",
        "        mask = cv2.resize(mask, (520, 520), interpolation=cv2.INTER_NEAREST)  # Resize mask (use nearest for binary mask)\n",
        "\n",
        "        # Normalize mask: 255 (lane) → 1, 0 (background) → 0\n",
        "        mask = (mask > 128).astype(np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = torch.tensor(mask, dtype=torch.long)  # Convert to tensor\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((520, 520)),  # Resize images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = KITTILaneDataset(\"./training/image_2\", \"./training/semantic_rgb\", transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrQTcMSqyern"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Cross-Entropy Loss for segmentation\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "Eo8ctdXHyhu0",
        "outputId": "9cdfb19a-e294-46c8-cc49-5a9fadcb338d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10  # Set epochs\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(dataloader, leave=True)\n",
        "\n",
        "    for images, masks in loop:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)[\"out\"]  # Forward pass\n",
        "\n",
        "        loss = criterion(outputs, masks)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        loop.set_postfix(loss=running_loss / len(dataloader))\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "i0QBmfhm0gP1",
        "outputId": "dd5991c1-50d3-40ad-abf6-47847731fc93"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def predict(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    input_image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_image)['out']\n",
        "        predicted_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(predicted_mask, cmap=\"gray\")\n",
        "    plt.title(\"Predicted Lane Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run prediction on a sample image\n",
        "predict(\"/content/testing/image_2/000001_10.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ee3JGUUc03OH",
        "outputId": "0299f6c6-6563-4efe-aae4-3fc9e8b0c402"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "im_path = \"/content/training/image_2/000020_10.png\"\n",
        "image = PIL.Image.open(im_path)\n",
        "plt.imshow(image)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "mask_path = \"/content/training/semantic_rgb/000020_10.png\"\n",
        "mask = PIL.Image.open(mask_path)\n",
        "plt.imshow(mask)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "UV3DdgZj1Ryx",
        "outputId": "72d4cb21-9d24-4cfc-f83a-59748e632080"
      },
      "outputs": [],
      "source": [
        "pixels = np.array(mask)\n",
        "lane_color = np.array([128,64,128])\n",
        "lane_mask = np.all(pixels==lane_color, axis=-1)\n",
        "\n",
        "pixels[lane_mask] = [255,255,255]\n",
        "pixels[~lane_mask]=[0,0,0]\n",
        "\n",
        "preprocessed_mask = PIL.Image.fromarray(pixels)\n",
        "plt.imshow(preprocessed_mask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0U3AKqGJ-5"
      },
      "source": [
        "###### To find the number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAvI3QQL3ehg"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "mask_path = \"/content/training/semantic_rgb/000020_10.png\"\n",
        "mask = cv2.imread(mask_path)\n",
        "mask_reshaped = mask.reshape(-1, 3)\n",
        "unique_classes = np.unique(mask_reshaped, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPNULYP1EfQM",
        "outputId": "e5f00607-ee20-446a-8723-e64f20881a86"
      },
      "outputs": [],
      "source": [
        "print(unique_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8xEY7q_EYl1",
        "outputId": "4d823cce-0f12-4de7-f786-d7c50a60a9ec"
      },
      "outputs": [],
      "source": [
        "print(len(unique_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxGzQsg0LoCA"
      },
      "source": [
        "###### To preprocess training masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt_4f_cWFTMk"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary mapping RGB colors to class labels\n",
        "# The key is the (R, G, B) tuple, and the value is the corresponding class label\n",
        "color_map = {\n",
        "    (128, 64, 128): 1,    # Road\n",
        "    (35,142,107): 2,    # Sidewalk\n",
        "    (70, 70, 70): 3,      # Building\n",
        "    (60,20,220): 4,   # Wall\n",
        "    (153, 153, 153): 5,   # Fence\n",
        "    (153, 153, 190): 6,   # Vegetation\n",
        "    (0,220,220): 7,     # Terrain\n",
        "    (142,0,0): 8,       # Sky\n",
        "    (100, 100, 150): 9,       # Person\n",
        "    (152, 251, 152): 10,      # Car\n",
        "    (180, 130, 70): 11,    # Bicycle\n",
        "    (232, 35, 244): 12,    # Motorcycle\n",
        "    (0,0,0): 0,   # Background (can be 0)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6PO4XOvJwwu",
        "outputId": "e21db557-34cb-4ae9-e67a-c06d9eb06357"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load a color-encoded mask image (example path)\n",
        "mask_image = cv2.imread('/content/training/semantic_rgb/000020_10.png')  # Shape should be (height, width, 3)\n",
        "\n",
        "# Create an empty integer mask with the same height and width as the input mask\n",
        "height, width, _ = mask_image.shape\n",
        "integer_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "# Iterate over each pixel in the mask and convert the RGB to class label\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        # Get the RGB value of the pixel\n",
        "        rgb_value = tuple(mask_image[i, j])\n",
        "\n",
        "        # If the RGB value exists in the color_map, assign the corresponding class label\n",
        "        if rgb_value in color_map:\n",
        "            integer_mask[i, j] = color_map[rgb_value]\n",
        "        else:\n",
        "            integer_mask[i, j] = 0  # If the color is not in the map, we assign it as background\n",
        "\n",
        "# Now `integer_mask` contains class labels (0 to 12 for KITTI)\n",
        "print(integer_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqnz8_00J51l",
        "outputId": "6570c30c-39ec-4221-c27e-d82b4b6110be"
      },
      "outputs": [],
      "source": [
        "cv2.imwrite('converted_mask.png', integer_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je12shPJKDlQ",
        "outputId": "cdc4480f-7d52-485f-8da0-d58be85acc10"
      },
      "outputs": [],
      "source": [
        "scaled_mask = (integer_mask * (255 // 12)).astype(np.uint8)\n",
        "cv2.imwrite('converted_scaled_mask.png', scaled_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jx2qORTUKads",
        "outputId": "1acb4ac7-8311-429b-d896-c925cd853ef5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder with RGB masks\n",
        "mask_folder = '/content/training/semantic_rgb'\n",
        "output_mask_folder = '/content/preprocessed_masks'\n",
        "os.makedirs(output_mask_folder)\n",
        "n = 0\n",
        "total = len(os.listdir(mask_folder))\n",
        "# Loop through all mask images\n",
        "for mask_filename in os.listdir(mask_folder):\n",
        "    if mask_filename.endswith(\".png\"):\n",
        "        mask_path = os.path.join(mask_folder, mask_filename)\n",
        "        mask_image = cv2.imread(mask_path)\n",
        "\n",
        "        # Create empty integer mask\n",
        "        height, width, _ = mask_image.shape\n",
        "        integer_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "        # Convert RGB to class labels\n",
        "        for i in range(height):\n",
        "            for j in range(width):\n",
        "                rgb_value = tuple(mask_image[i, j])\n",
        "                if rgb_value in color_map:\n",
        "                    integer_mask[i, j] = color_map[rgb_value]\n",
        "                else:\n",
        "                    integer_mask[i, j] = 0  # Background\n",
        "\n",
        "        # Save the integer mask\n",
        "        output_mask_path = os.path.join(output_mask_folder, mask_filename)\n",
        "        cv2.imwrite(output_mask_path, integer_mask)\n",
        "        n+=1\n",
        "        print(str(n) +'/'+ str(total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGcElRNLXsnz"
      },
      "source": [
        "###### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM3buPuVSTmw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class KITTIDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, image_size=(256, 256)):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_size = image_size\n",
        "        self.image_names = sorted(os.listdir(image_dir))\n",
        "        self.mask_names = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_names[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_names[idx])\n",
        "\n",
        "        # Open the image and mask using PIL\n",
        "        image = Image.open(image_path)\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        image = image.resize(self.image_size, Image.BILINEAR)\n",
        "        mask = mask.resize(self.image_size, Image.NEAREST)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = torch.tensor(np.array(mask), dtype=torch.long)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWxH4X9wX2y-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transformations (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize all images to 256x256 (adjust as needed)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = KITTIDataset(image_dir='/content/training/image_2', mask_dir='/content/preprocessed_masks', transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kjKLBWKgYDzb",
        "outputId": "05440da3-cce5-4afc-dca0-71e5b2dc2739"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models.segmentation as segmentation\n",
        "\n",
        "# Load pre-trained DeepLabV3+ with a ResNet-101 backbone\n",
        "model = segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "\n",
        "# Modify the classifier to output 13 classes (for KITTI dataset)\n",
        "model.classifier[4] = torch.nn.Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CILx-HaoYGwn"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Loss function: CrossEntropyLoss (for multi-class segmentation)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "EygaD5YDYKLx",
        "outputId": "17c0b2e8-278f-4ae9-b6da-bc13e44db199"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10  # Adjust number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)['out']\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Optionally, you can save the model after every epoch or at specific intervals\n",
        "torch.save(model.state_dict(), f\"deeplabv3_epoch{epoch+1}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpFgtl3TgVxG",
        "outputId": "fd183c45-d94c-4677-a01f-0744b37aa0cb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "num_epochs = 10  # Adjust number of epochs\n",
        "print_freq = 10  # Print loss every 10 batches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # tqdm progress bar for better visibility\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (images, masks) in progress_bar:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)['out']  # Forward pass\n",
        "        loss = criterion(outputs, masks)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Optimize the weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print progress every `print_freq` batches\n",
        "        if (batch_idx + 1) % print_freq == 0 or batch_idx == len(train_loader) - 1:\n",
        "            avg_loss = running_loss / (batch_idx + 1)\n",
        "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
        "\n",
        "    # Calculate and print epoch summary\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f} - Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save model at the end\n",
        "torch.save(model.state_dict(), f\"deeplabv3_epoch{epoch+1}.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1rJzecc3z_"
      },
      "source": [
        "###### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "i01PuqOIYRuN",
        "outputId": "421a9a6a-e332-4c89-f5fd-563c721ef7c2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict and visualize\n",
        "model.eval()\n",
        "image, mask = train_dataset[0]  # Take an image from the dataset\n",
        "\n",
        "# Get model prediction\n",
        "with torch.no_grad():\n",
        "    output = model(image.unsqueeze(0).to(device))['out'][0]\n",
        "    predicted_mask = torch.argmax(output, dim=0).cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406])  # ImageNet mean\n",
        "std = np.array([0.229, 0.224, 0.225])   # ImageNet std\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\" Convert normalized tensor back to original image format \"\"\"\n",
        "    tensor = tensor.numpy().transpose(1, 2, 0)  # Convert [C, H, W] to [H, W, C]\n",
        "    tensor = (tensor * std) + mean  # Undo normalization\n",
        "    tensor = np.clip(tensor, 0, 1)  # Clip values to be between [0,1]\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# Original Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(denormalize(image.cpu()))\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Predicted Mask\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow((predicted_mask * (255 // 12)).astype(np.uint8))\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# True Mask\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(mask)\n",
        "plt.title(\"True Mask\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCEjuPK1c9eo"
      },
      "source": [
        "###### Save the model to my Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQIbyIxZ8SY",
        "outputId": "b3428ebf-efce-43f6-c015-431aa9881ecf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Gives access to your Google Drive\n",
        "torch.save(model.state_dict(), \"/content/drive/My Drive/deeplabv3_epoch10.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLrxnx13aTnL"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/preprocessed_masks\" \"/content/drive/MyDrive/MECH 501 Lane Segmentation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odw3r2oOdBvO"
      },
      "source": [
        "###### Define function to evaluate IoU metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3JBJp3QbT3C"
      },
      "outputs": [],
      "source": [
        "def compute_iou(pred_mask, true_mask, class_id):\n",
        "    \"\"\"\n",
        "    Computes the Intersection over Union (IoU) for a specific class in a single image.\n",
        "\n",
        "    Args:\n",
        "    - pred_mask (numpy array): Predicted segmentation mask\n",
        "    - true_mask (numpy array): Ground truth segmentation mask\n",
        "    - class_id (int): The class ID to compute IoU for (e.g., lane markings).\n",
        "\n",
        "    Returns:\n",
        "    - IoU score (float)\n",
        "    \"\"\"\n",
        "\n",
        "    assert pred_mask.shape == true_mask.shape, \"Mask shapes should be the same\"\n",
        "\n",
        "    assert class_id in color_map.values(), \"class_ID should be defined in color to class mapping\"\n",
        "\n",
        "    # Create binary masks for the given class - Extract the class required\n",
        "    pred_class = (pred_mask == class_id)\n",
        "    true_class = (true_mask == class_id)\n",
        "\n",
        "    # Compute intersection and union\n",
        "    intersection = np.logical_and(pred_class, true_class).sum()\n",
        "    union = np.logical_or(pred_class, true_class).sum()\n",
        "\n",
        "    if union == 0: #If the required class does not exist in either of the prediction or true mask\n",
        "        return 0.0\n",
        "\n",
        "    return intersection / union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-IZcf2CdJ6a"
      },
      "source": [
        "###### Compute IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Bu1lX_cZ4Y"
      },
      "outputs": [],
      "source": [
        "iou_score = compute_iou(predicted_mask, mask, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlFxEcM7ddAP",
        "outputId": "7bed36fc-bfa8-4a83-f7b4-5d71b139daf5"
      },
      "outputs": [],
      "source": [
        "iou_score.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9boXxY0nlXeV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
